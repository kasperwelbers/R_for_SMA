---
title: 'Social Media Analytics V'
subtitle: "Text analysis in R"
output:
  html_document:
    theme: united
    highlight: tango
    css: custom.css
    toc: true
    toc_depth: 1
editor_options:
  chunk_output_type: console
---


```{r opts, echo = FALSE}
knitr::opts_chunk$set(results='hide', message=FALSE, warning=FALSE)
```

VU Amsterdam

# This practical

In this practical you will learn how to import texts into R and use a number of text analysis techniques to explore and visualize the content.
The first part of this practical is mostly a tutorial.
The second part contains the assignments, but these will be hard to make if you did not complete the tutorial.


# Reading CSV files into R

In this tutorial we'll use CSV files to load data into R.
`CSV` stands for Comma Separated Values, and it's is one of the most common formats for storing data in a `data frame` format. 
That is, data in which we have rows and columns.
You are probably most familiar with this type of data representation from Excel or SPSS. 
In the most typical use case, rows represent cases in your data (e.g., survey participants) and columns represent variables (e.g., age, gender).
In our current case, each row in our data is a tweet, and columns contain information about this tweet, such as the author and the text.

So why do we use `csv` files instead of something like Excel? 
The main reason is that a `csv` file is plain and simple.
Excel files have all sorts of bells and whistles, which makes them nice to work with, but also prone to get messy when we just want to store data.

There are two important pitfalls to avoid when working with CSV files:

* When you try to download the file from Canvas, some computers might immediately ask you whether you want to open it in Excel (or a similar program on Mac). Do not do this, but instead download the file directly to your computer. If you open the file in Excel and then save it, it might already have messed up the data.
* There are different types of CSV files (yes, life is complicated). Most importantly, there are two common versions that you should know about. In this tutorial we'll only use the most common one, and we'll use the other one next time. 

For reading csv files into R, we're going to use the `read_csv` function from the `readr` package.
R also has a built-in function called `read.csv` (with a dot instead of an underscore), but the `read_csv` function from the `readr` package is faster and overall nicer. 
To use the readr package, we're going to install tidyverse. This is a collection of packages that includes `readr`,
but also many other packages that are useful for data analysis. 


```{r, eval=F}
install.packages('tidyverse')
```

And now we can use `library` to use the package in our current session.

```{r}
library(tidyverse)
```

### Importing the tweets_rstats.csv file

We're going to import the CSV file called `tweets_rstats.csv` that you can find on Canvas under this week's assignment.

You can use the `read_csv` function by providing it with a string for the location of the file on your computer.
This will look something like the following code, but you'll have to replace the string with the location on your own computer.
If you can't figure out how to do this, we provide an alternative solution in the next subsection!

```{r}
tweets = read_csv('~/projects/R_for_SMA/data/tweets_rstats.csv')
```

### Using RStudio to import the file 

Some of you might not be familiar with this way of specifying the location of a file.
Don't worry though! RStudio can help you with that.
In the bottom-right window, you can go to the `Files` tab.
Here you can browse to the file on your computer. 
Then when you click on the file, RStudio will understand that it's a CSV file, and give you the `Import Dataset` option. 
If you click "Import Dataset", then you'll get a window for importing the data.

```{r, echo=F, out.width="70%", fig.align='center', fig.width=15, fig.fullwidth=TRUE}
knitr::include_graphics('img/import_data.png')
```

DO NOT YET CLICK ON THE `IMPORT` BUTTON!
This would import the file, but the better approach is to copy the code for importing the file and doing it yourself.
At the top of the window you'll see a Data Preview, which tells you how it would import the data frame.
Since tweets_rstats.csv follows the standard format, this should all look good. 
At the bottom (right) you should see the `Code Preview`. 
Copy this code and close the window by pressing the `Cancel` button.

Now you can copy this code into R, which will look something like this, but the part between quotes should now contain the location of the file on your own computer.

```{r, eval=F}
library(readr)
tweets_rstats <- read_csv('~/projects/r_for_sma/data/tweets_rstats.csv')
View(tweets_rstats)
```

Only the second line is really important here (assuming you already opened the `readr` package earlier). 
Note that by default RStudio will import the data using the name of the file, which in this case is `tweets_rstats`.
So for now you'll want to change that into `tweets`, because that's how we called it above.
(So not `tweets_rstats <- ...` but `tweets <- ...`)

### Look at the Twitter data

Now that you've imported the Twitter data, a good first step is to have a good look.
You'll need to know a bit about what types of information you have about each tweet.
A good way to do this is by using the `View` function (what RStudio also gave you when importing the data).

```{r, eval=F}
View(tweets)
```

Looking at the data, you'll see that for every tweet we have the text and engagement statistics (likes, shares, etc.), and some other stuff that we won't use here (created_at, author_id).

# Analyzing the Tweets data

We'll first re-cap the techniques that you worked with before, but this time using our tweets dataset (so make sure to follow the steps above for importing the tweets data)
This time, we'll take some more time to talk about what is actually happening in the code.
The first step is to open the required libraries.

```{r}
library(quanteda)             
library(quanteda.textstats)   
library(quanteda.textplots)
```

Last time we used the `data_corpus_inaugural` data. This is a demo dataset included in `quanteda`.
This time, we bring our own tweets data.
Our first step is to tell R that our data.frame of Tweets can be seen as a `corpus` (i.e. a collection of texts).

### Creating the corpus 

We can create a corpus with the aptly named `corpus` function.
The only thing we need to add in this function, is that we tell it that the `text` column in the twitter data contains the text.

```{r}
corp = corpus(tweets, text_field = 'text')    
corp
```

Looking at the corpus object, we see that it has 50,000 documents (tweets) and 8 document variables (docvars).
Remember that you can use the `docvars(corp)` function to view the document variables.
This would show you that for every tweet we know some engagement statistics (likes, retweets, replies) and more.

### Tokenizing the corpus 

Last time we saw that we can create a wordcloud from a corpus. 
Let's do this again, but take it a bit more slowly to see what is actually happening.
The first thing we did was use the `tokens()` function. 
This function `tokenizes` the text, which simply means that it breaks the text up into words.
We can also provide some additional arguments, such as removing the punctuation (dots, commas, etc.).

```{r}
tok = tokens(corp, remove_punct=T)
tok
```

When we look at `tok` we see the tokenized corpus. 
It looks pretty similar to `corp`, but this time the texts have been split into words.
**Tokenization** is a fundamental step in text analysis. Computers cannot understand language, but they are very good at counting words. 

### Creating the Document Term Matrix

The next step, that we also performed last time, is to create a `Document Term Matrix` (DTM).
This is a matrix in which the rows are documents, the columns are words, and cells indicate how often each word occurred in each document.
In `quanteda` we create this matrix with the `dfm` function. 
In case you are wondering why the function isn't called `dtm`: quanteda uses a slightly more general (but less common) term of a `Document Feature Matrix`. 

```{r}
dtm = dfm(tok)
dtm
```

When we look at the `dtm` we see something similar to the tokenized corpus `tok`. 
We still have our 50,000 documents and 8 docvars, and the texts are still broken into single words, but this time we have counted the words per document.
This has two major implication:

* The cool thing is that we have now successfully transformed texts into numbers that we can perform calculations with! We can sum up the numbers in the columns to get the total frequencies that we need to create a wordcloud. We can also use statistics like cosine similarities (similar to correlations) to see which words and documents are similar. And many more things. 
* The not so cool thing is that we have just thrown away any information about the order of words. We now longer know that the word "Society" came directly after the word "French" in the first document. We only know how often both words occurred.

This second implication is **very important to understand**, because it tells you something about what you can and cannot do with a DTM.
It turns out that even without knowing the positions of words, we can still do many useful things.
For example, the mere frequencies of words are often sufficient for determining the topic of a text.
But imagine that you have the text "Steve was angry at Bob". 
After transforming this text into a DTM, it becomes impossible to say whether Steve was angry at Bob, or that it was Bob who was angry at Steve.

To remind us of this limitation of the DTM representation, it is also referred to as a **bag-of-words** representation.
The take-home message is that DTM's are very useful for performing automatic content analysis, but certain types of content analysis are not accurately possible with them. 

### Preprocessing 

In the current DTM we still included all words as they originally occurred in the corpus.
This has important consequences:

* Some words are much more relevant than others for certain types of analysis. If we want to learn something about the topic of a text, we are for instance not interested in very common words like `the`, `it` and `to`. These types of words are also referred to as `stopwords`. Last time, we therefore removed these stopwords before creating a wordcloud.
* Our DTM does not know anything *about* the words, so it doesn't understand that words like `cat` and `cats` have a very similar meaning. A simplistic but sometimes effective solution is to `stem` the words. This tries to reduce words to their `stem`, so that `cats` becomes `cat`. However, simple stemming algorithms are pretty dumb, so they also mangle words like `societies` into `societi`. 

You already saw how to remove stopwords last time. 
The `dfm_remove(dtm, [words to remove])` function removes words from the DTM.
The `stopwords("en")` function creates a list of English stopwords ("it", "the", etc.).
So `dfm_remove(dtm, stopwords('en'))` removes this list of stopwords.
Look close at the columns in the result to see that the stopwords are indeed gone.

```{r}
dtm = dfm_remove(dtm, stopwords('en'))
dtm
```

Stemming is just as easy. Just use the `dfm_wordstem` function. Notice that some words now look rather ugly, but the total number of unique features (i.e. words) is reduced from 90,814 to 84,231. 

```{r}
dtm = dfm_wordstem(dtm)
dtm
```

### Creating the wordcloud

Now we are finally ready to create the wordcloud.

```{r}
textplot_wordcloud(dtm, max_words = 40)
```

We see that this wordcloud mainly contains hashtags.
This makes sense, because hashtags are designed to make people use the same terms.

You can also remove the hashtags. 
Here we again use the `dfm_remove` function, but this time instead of providing a list of stopwords, we provide the string `"#*"`.
The `*` here is a wildcard, just as you saw in the lesson on Boolean queries.
So what we're saying here is: remove all words that start with a hashtag.

```{r}
dtm_no_hashtag = dfm_remove(dtm, '#*')
textplot_wordcloud(dtm_no_hashtag, max_words = 40)
```

### Exploring the context of words

One of the limitations of wordclouds is that they only show you the frequencies of words.
They don't tell you anything about how these words are related, or in what context they are used.
As a visualization of a DTM, it can only visualize the `bag-of-words`.

A solution is to use the `kwic` function, which stands for `key-word-in-context`.
The first argument to this function is the tokenized corpus (that we called `tok` above).
The second argument is a string (a word between quotes) for a word that you want to examine.
This string can also have the `*` wildcard, that you also used in the lesson about Boolean queries.
The following code creates a `key-word-in-context` listing for all occurences of words that start with "package" (e.g., package, packages, package's).
We then use `head(kw, 10)` to only print the first 10 rows (you can change the number for more rows if you want).

```{r}
kw = kwic(tok, "package")
head(kw, 10)
```

You should now see the rows that look like this:

> [text47, 12]   putting off making an#rstats     |    package    | that would just make more                                             
> [text76, 7]    Data Wrangling and Data Cleaning |   Packages    | for R in 2023 A     

If this is not the case, your console panel (the bottom-left panel) might not be wide enough. 
If so, try making it wider (you can drag the borders of the RStudio panels)
Alternatively, you could use View to explore the `kwic` results as a data.frame

```{r, eval=F}
View(kw)
```

You then see the columns `pre`, `keyword` and `post` for the text before (pre) and after (post) the keyword.

# Assignment 1: Analyzing Twitter

For this assignment we use the `tweets_rstats.csv` data, just as in the `Analyzing the tweets data` section above.
You can find this data on Canvas under this weeks assignment.

> **Question 1.a**.
> Create a wordcloud of the tweets_rstats.csv data that **includes** the hashtags. DO remove stopwords, but DO NOT use stemming. Include the code and the wordcloud itself.

> **Question 1.b**.
> Interpret the wordcloud from **1.a**. What does this tell you about the tweets that mention `rstat`?

> **Question 1.c**.
> Create the wordcloud again, but this time **exclude** the hashtags. Include the code and the wordcloud itself.

> **Question 1.d**.
> Create key-word-in-context listings for two words from the wordcloud in question **1.c** (without the hashtags). Use the `head` function to only show the top 10 results. Include both listings in your answer, and provide your own interpretation of how the words were used.

Last time we used `dfm_subset` to make a wordcloud just about Obama (`President == Obama`). 
In the current data we also have information about the number of likes for each tweet, which we can use to subset data on only tweets that received at least 100 likes (`likes >= 100`).

> **Question 1.f**.
> Re-create the wordcloud from **1.a** that only uses tweets with at least 100 likes. Include the code and the wordcloud in your answer.



# Writing better code with pipes

When we create a DTM, it's quite inconvenient that we need to first create a corpus, then create the tokens, and then create the DTM.
In particular, it feels unnecessary that we *have* to give names to the corpus and the tokens if we're only interested in the DTM.
And indeed, this actually is unnecessary, and can be avoided by working with `pipes`.

Look again at the following code.

```{r}
corp = corpus(tweets, text_field="text")
tok = tokens(corp)
dtm = dfm(tok)
```

Ideally, we would just provide the `tweets` as input and receive the `dtm` as output, without having to create `corp` and `tok`.
This can be achieved with the `%>%` (pipe) operator, which allows us to create a pipeline of functions.

```{r}
dtm = tweets %>% 
  corpus(text_field="text") %>% 
  tokens() %>% 
  dfm()
```

With this notation, it is immediately clear that these lines of code belong together as a single operation, and no needless names are created.

You can even put parts of the pipe on the same line if you like.

```{r}
dtm = tweets %>% corpus(text_field="text") %>% tokens() %>% dfm()
```

So how does this work? Basically, you should read the pipe operator as `input %>% function`.
The data that comes before the `%>%` operator will be plugged into the function *as the first argument*.
Here are some examples.

```{r}
input = "Example text."

## Pass input to the tokens function
tokens(input)
## Do the exact same thing with a pipe
input %>% tokens()

## Pass input to the tokens function with an additional argument
tokens(input, remove_punct = T)
## Do the exact same thing with a pipe
input %>% tokens(remove_punct = T)
```

You might be wondering why we have two ways of doing the same thing.
The reason is that readability of code is just as important as functionality.
The pipe is not meant to tell R what to do.
The pipe is meant to make it more obvious to a human what the code is doing.
In programming this is also called `syntactic sugar`.

We avoided talking about pipes until now because when you're just getting started 
it's annoying if you have to learn multiple ways to do the same thing. But pipes
can really help you write better code, and if you look up code online you will often
see people using pipes. So it's best to learn it sooner rather than later.

# Assignment 2: Working with pipes

> **Question 2.a**.
> Rewrite the code from question **1.a** using the pipe syntax. 

> **Question 2.b**.
> Rewrite the code from question **1.c** using the pipe syntax.


# Assignment 3: Putting it all together   

In this final assignment you're going to use the techniques from the previous assignments to analyze a new set of tweets data. The data you will be using is also available on Canvas, and is called `machine_learning_tweets.zip`. This is a zip file, so you'll first need to unzip it. This will give you a folder with four CSV files. 

Each of these CSV files contains tweets for training a machine learning model. There are two columns: "text" and "label". 
The label is basically how the tweet is 'coded'. For example, in the `offensive_tweets.csv` file, the label is either "offensive" or "not-offensive".
This `training data` allows us to train a machine learning model to automatically detect whether a tweet is offensive or not.
In this final assignment we'll take a look at this data to get a better idea of how a computer might learn to do this. (in the next practical we'll actually train the model)

The following code shows you can split the data in two parts: one with offensive tweets and one with non-offensive tweets.
Here we are again using `read_csv()` to import the data, and we're using the `filter()` function to filter the data based on the value
in the label column.

```{r}
offensive_tweets = read_csv('~/Downloads/machine_learning_tweets/offensive_tweets.csv')

offensive = filter(offensive_tweets, label == "offensive")
not_offensive = filter(offensive_tweets, label == 'not-offensive')
```

For the assignment you need to do the same, but this time for the `hate_tweets.csv` data. 
So you will read the data into R and create two dataframes: 

* **hate** contains the tweets that are labeled as "hate"
* **not_hate** contains the tweets that are labeled as "not-hate"

Now use this data to answer the following questions.

> **Question 3.a**.
> Create a wordcloud for **only the "hate" tweets**. Provide the code for creating the wordcloud. 
> **You are required** to use the pipe syntax for this question. Also make sure to use sensible pre-processing steps.

> **Question 3.b**.
> Describe in your own words what each step in your code does. Imagine you're explaning it to someone that doesn't know R or text analysis, 
> so you need to provide some context for each step. (e.g., not just: "this creates a DTM". Say something about a DTM is).

> **Question 3.c**.
> Now use the exact same code to create a wordcloud for the "not-hate" tweets. i.e. only change the data that you're using.

> **Question 3.d**.
> Compare the wordclouds from **3.a** and **3.c**. What do you notice? 

> **Question 3.e**.
> A collegue asks you if you can explain to him how Supervised Machine Learning works. Provide an explanation in which you use
> the two wordclouds from the hate_tweets training data to explain this to him.

