---
title: 'Social Media Analytics IV'
subtitle: "Text analysis in R"
output:
  html_document:
    theme: united
    highlight: tango
    css: custom.css
    toc: true
    toc_depth: 1
editor_options:
  chunk_output_type: console
---


```{r opts, echo = FALSE}
knitr::opts_chunk$set(results='hide', message=FALSE, warning=FALSE)
```

VU Amsterdam

# This practical

In this practical you will learn how to import texts into R and use a number of text analysis techniques to explore and visualize the content.
The first part of this practical is mostly a tutorial.
The second part contains the assignments, but these will be hard to make if you did not complete the tutorial.


# Gathering Social Media data

The first step in social media analysis is to collect the data.
The good news is that you can also use R as a data collection tool!
The bad news is that data collection can be expensive or impossible for certain platforms.
An important element of social media analytics is therefore to know what data collection options are available.

There are two common ways to collect social media data:

* using an API
* using web scraping

### API

**API** stands for Application Programming Interface. Simply put, an API makes it easier for different application to talk to each other. For example, if you have an application on your phone for editing photo's that also allows you to immediately post these photo's on Twitter, this application communicates with Twitter via the Twitter API. 

The reason APIs are relevant for Social Media Analytics is that you can often also use an API to collect data. For example, Twitter has an [Academic Research API](https://developer.twitter.com/en/products/twitter-api/academic-research) that allows researchers to query 10 million tweets per month from the entire Twitter archive. 
Similar types of API access exist for other social media platforms.
The limitation is that you can only collect data that the platform gives you access to, and often you need to pay to get access to the API.

Here is an example of how the Twitter API would work. Do not try to run this example yourself, because you would need to register for the Academic Twitter API. This is only possible for Master students or University employees. Sadly, it's also possible that the Twitter Academic API will get closed done soon or at least no longer be free to use, because Twitter is changing a lot now that it's owned by Elon Musk. The following example is just to demonstrate how API's can make data collection easy. One of the reasons R is so popular for data science is that there are often packages for working with these APIs  

```{r, eval=F}
# don't try to run this code, because it doesn't work if you're not registered for the Academic Twitter API.
library(academictwitteR)

# This code collects the most recent 50,000 tweets about rstats (a common way to refer to R) in 2023.
tweets = get_all_tweets(
  query = "rstats",
  start_tweets = '2023-01-01T00:00:00Z',
  end_tweets = '2023-05-05T00:00:00Z',
  data_path= "data/rstats",
  lang='en',
  n=50000,
  is_retweet = FALSE,
)
```

### Web Scraping

Another method for collecting data from the internet is to create a robot that visits websites and collects the data for you. This is a more time consuming method than using an API (and can have some ethical and legal issues), but it allows you to collect data if no API is available. These robots are often called **Web Scrapers**. You can create your own web scraper in R, but this does require a bit more experience in R than we cover in this course (Though if you're interested, we do have an (introduction video)[https://www.youtube.com/watch?v=9GR26Y4z_v4]).

There are also packages in R that can do the web scraping for you. For instance, the RedditExtractoR let's you search for threads and then collect all comments in the thread. You can run the code below yourself. Just remember to run `install.packages("RedditExtractoR")` first.

```{r, eval=F}
library(RedditExtractoR)  

## search for threads about rstats in the last week (a common way to refer to R)
threads = find_thread_urls("rstats", period='week')
View(threads)
threads$title

## get thread and comments for the first thread that we found
first_thread = threads$url[1]
content = get_thread_content(first_thread)

View(content$threads)
View(content$comments)
```

# Reading CSV files into R

In this course we will provide the Social Media data using `CSV` files.
`CSV` stands for Comma Separated Values, and it's is one of the most common formats for storing data in a `data frame` format. 
That is, data in which we have rows and columns.
You are probably most familiar with this type of data representation from Excel or SPSS. 
In the most typical use case, rows represent cases in your data (e.g., survey participants) and columns represent variables (e.g., age, gender).
In our current case, each row in our data is a tweet, and columns contain information about this tweet, such as the author and the text.

So why do we use `csv` files instead of something like Excel? 
The main reason is that a `csv` file is plain and simple.
Excel files have all sorts of bells and whistles, which makes them nice to work with, but also prone to get messy when we just want to store data.

* When you try to download the file from Canvas, some computers might immediately ask you whether you want to open it in Excel (or a similar program on Mac). Do not do this, but instead download the file directly to your computer. If you open the file in Excel and then save it, it might already have messed up the data.
* There are different types of CSV files (yes, life is complicated). Most importantly, there are two common versions that you should know about. In this tutorial we'll only use the most common one, and we'll use the other one next time. 

For reading csv files into R, we're going to use the `read_csv` function from the `readr` package.
R also has a built-in function called `read.csv` (with a dot instead of an underscore), but the `read_csv` function from the `readr` package is faster and overall nicer. 
We'll first need to install `readr` (remember that you only need to do this once)

```{r, eval=F}
install.packages('readr')
```

And now we can use `library` to use the package in our current session.

```{r}
library(readr)
```

### Importing the tweets_rstats.csv file

We're going to import the CSV file called `tweets_rstats.csv` that you can find on Canvas under this week's assignment.
This is the data that we collected from Twitter using the Academic API, as shown above.

You can use the `read_csv` function by providing it with a string for the location of the file on your computer.
This will look something like the following code, but you'll have to replace the string with the location on your own computer.
If you can't figure out how to do this, keep reading, because we'll provide an alternative solution!

```{r}
tweets = read_csv('~/projects/r_for_sma/data/tweets_rstats.csv')
```

### Using RStudio to import the file 

Many of you are probably not familiar with this way of specifying the location of a file.
Don't worry though! RStudio can help you with that.
In the bottom-right window, you can go to the `Files` tab.
Here you can browse to the file on your computer. 
Then when you click on the file, RStudio will understand that it's a CSV file, and give you the `Import Dataset` option. 
If you click "Import Dataset", then you'll get a window for importing the data.

```{r, echo=F, out.width="70%", fig.align='center', fig.width=15, fig.fullwidth=TRUE}
knitr::include_graphics('img/import_data.png')
```

DO NOT YET CLICK ON THE `IMPORT` BUTTON!
This would import the file, but the better approach is to copy the code for importing the file and doing it yourself.
At the top of the window you'll see a Data Preview, which tells you how it would import the data frame.
Since tweets_rstats.csv follows the standard format, this should all look good. 
At the bottom (right) you should see the `Code Preview`. 
Copy this code and close the window by pressing the `Cancel` button.

Now you can copy this code into R, which will look something like this, but the part between quotes should now contain the location of the file on your own computer.

```{r, eval=F}
library(readr)
tweets_rstats <-read_csv('~/projects/r_for_sma/data/tweets_rstats.csv')
View(tweets_rstats)
```

Only the second line is really important here (assuming you already opened the `readr` package earlier). 
Note that by default RStudio will import the data using the name of the file, which in this case is `tweets_rstats`.
So for now you'll want to change that into `tweets`, because that's how we called it above.
(So not `tweets_rstats <- ...` but `tweets <- ...`)

### Look at the Twitter data

Now that you've imported the Twitter data, a good first step is to have a good look.
You'll need to know a bit about what types of information you have about each tweet.
A good way to do this is by using the `View` function (what RStudio also gave you when importing the data).

```{r, eval=F}
View(tweets)
```

Looking at the data, you'll see that for every tweet we have the text and engagement statistics (likes, shares, etc.), and some other stuff that we won't use here (created_at, author_id).

# Analyzing the Tweets data

We'll first re-cap the techniques that you worked with before, but this time using our tweets dataset (so make sure to follow the steps above for importing the tweets data)
This time, we'll take some more time to talk about what is actually happening in the code.
The first step is to open the required libraries.

```{r}
library(quanteda)             
library(quanteda.textstats)   
library(quanteda.textplots)
```

Last time we used the `data_corpus_inaugural` data. This is a demo dataset included in `quanteda`.
This time, we bring our own tweets data.
Our first step is to tell R that our data.frame of Tweets can be seen as a `corpus` (i.e. a collection of texts).

### Creating the corpus 

We can create a corpus with the aptly named `corpus` function.
The only thing we need to add in this function, is that we tell it that the `text` column in the twitter data contains the text.

```{r}
corp = corpus(tweets, text_field = 'text')    
corp
```

Looking at the corpus object, we see that it has 50,000 documents (tweets) and 8 document variables (docvars).
Remember that you can use the `docvars(corp)` function to view the document variables.
This would show you that for every tweet we know some engagement statistics (likes, retweets, replies) and more.

### Tokenizing the corpus 

Last time we saw that we can create a wordcloud from a corpus. 
Let's do this again, but take it a bit more slowly to see what is actually happening.
The first thing we did was use the `tokens()` function. 
This function `tokenizes` the text, which simply means that it breaks the text up into words.
We can also provide some additional arguments, such as removing the punctuation (dots, commas, etc.).

```{r}
tok = tokens(corp, remove_punct=T)
tok
```

When we look at `tok` we see the tokenized corpus. 
It looks pretty similar to `corp`, but this time the texts have been split into words.
**Tokenization** is a fundamental step in text analysis. Computers cannot understand language, but they are very good at counting words. 

### Creating the Document Term Matrix

The next step, that we also performed last time, is to create a `Document Term Matrix` (DTM).
This is a matrix in which the rows are documents, the columns are words, and cells indicate how often each word occurred in each document.
In `quanteda` we create this matrix with the `dfm` function. 
In case you are wondering why the function isn't called `dtm`: quanteda uses a slightly more general (but less common) term of a `Document Feature Matrix`. 

```{r}
dtm = dfm(tok)
dtm
```

When we look at the `dtm` we see something similar to the tokenized corpus `tok`. 
We still have our 50,000 documents and 8 docvars, and the texts are still broken into single words, but this time we have counted the words per document.
This has two major implication:

* The cool thing is that we have now successfully transformed texts into numbers that we can perform calculations with! We can sum up the numbers in the columns to get the total frequencies that we need to create a wordcloud. We can also use statistics like cosine similarities (similar to correlations) to see which words and documents are similar. And many more things. 
* The not so cool thing is that we have just thrown away any information about the order of words. We now longer know that the word "Society" came directly after the word "French" in the first document. We only know how often both words occurred.

This second implication is **very important to understand**, because it tells you something about what you can and cannot do with a DTM.
It turns out that even without knowing the positions of words, we can still do many useful things.
For example, the mere frequencies of words are often sufficient for determining the topic of a text.
But imagine that you have the text "Steve was angry at Bob". 
After transforming this text into a DTM, it becomes impossible to say whether Steve was angry at Bob, or that it was Bob who was angry at Steve.

To remind us of this limitation of the DTM representation, it is also referred to as a **bag-of-words** representation.
The take-home message is that DTM's are very useful for performing automatic content analysis, but certain types of content analysis are not accurately possible with them. 

### Preprocessing 

In the current DTM we still included all words as they originally occurred in the corpus.
This has important consequences:

* Some words are much more relevant than others for certain types of analysis. If we want to learn something about the topic of a text, we are for instance not interested in very common words like `the`, `it` and `to`. These types of words are also referred to as `stopwords`. Last time, we therefore removed these stopwords before creating a wordcloud.
* Our DTM does not know anything *about* the words, so it doesn't understand that words like `cat` and `cats` have a very similar meaning. A simplistic but sometimes effective solution is to `stem` the words. This tries to reduce words to their `stem`, so that `cats` becomes `cat`. However, simple stemming algorithms are pretty dumb, so they also mangle words like `societies` into `societi`. 

You already saw how to remove stopwords last time. 
The `dfm_remove(dtm, [words to remove])` function removes words from the DTM.
The `stopwords("en")` function creates a list of English stopwords ("it", "the", etc.).
So `dfm_remove(dtm, stopwords('en'))` removes this list of stopwords.
Look close at the columns in the result to see that the stopwords are indeed gone.

```{r}
dtm = dfm_remove(dtm, stopwords('en'))
dtm
```

Stemming is just as easy. Just use the `dfm_wordstem` function. Notice that some words now look rather ugly, but the total number of unique features (i.e. words) is reduced from 90,814 to 84,231. 

```{r}
dtm = dfm_wordstem(dtm)
dtm
```

### Creating the wordcloud

Now we are finally ready to create the wordcloud.

```{r}
textplot_wordcloud(dtm, max_words = 40)
```

We see that this wordcloud mainly contains hashtags.
This makes sense, because hashtags are designed to make people use the same terms.

You can also remove the hashtags. 
Here we again use the `dfm_remove` function, but this time instead of providing a list of stopwords, we provide the string `"#*"`.
The `*` here is a wildcard, just as you saw in the lesson on Boolean queries.
So what we're saying here is: remove all words that start with a hashtag.

```{r}
dtm_no_hashtag = dfm_remove(dtm, '#*')
textplot_wordcloud(dtm_no_hashtag, max_words = 40)
```

### Exploring the context of words

One of the limitations of wordclouds is that they only show you the frequencies of words.
They don't tell you anything about how these words are related, or in what context they are used.
As a visualization of a DTM, it can only visualize the `bag-of-words`.

A solution is to use the `kwic` function, which stands for `key-word-in-context`.
The first argument to this function is the tokenized corpus (that we called `tok` above).
The second argument is a string (a word between quotes) for a word that you want to examine.
This string can also have the `*` wildcard, that you also used in the lesson about Boolean queries.
The following code creates a `key-word-in-context` listing for all occurences of words that start with "package" (e.g., package, packages, package's).
We then use `head(kw, 10)` to only print the first 10 rows (you can change the number for more rows if you want).

```{r}
kw = kwic(tok, "package")
head(kw, 10)
```

You should now see the rows that look like this:

> [text47, 12]   putting off making an#rstats     |    package    | that would just make more                                             
> [text76, 7]    Data Wrangling and Data Cleaning |   Packages    | for R in 2023 A     

If this is not the case, your console panel (the bottom-left panel) might not be wide enough. 
If so, try making it wider (you can drag the borders of the RStudio panels)
Alternatively, you could use View to explore the `kwic` results as a data.frame

```{r, eval=F}
View(kw)
```

You then see the columns `pre`, `keyword` and `post` for the text before (pre) and after (post) the keyword.

# Assignment 1: Analyzing Twitter

For this assignment we use the `tweets_rstats.csv` data, just as in the `Analyzing the tweets data` section above.
You can find this data on Canvas under this weeks assignment.

> **Question 1.a**.
> Create a wordcloud of the tweets_rstats.csv data that **includes** the hashtags. DO remove stopwords, but DO NOT use stemming. Include the code and the wordcloud itself.

> **Question 1.b**.
> Interpret the wordcloud from **1.a**. What does this tell you about the tweets that mention `rstat`?

> **Question 1.c**.
> Create the wordcloud again, but this time **exclude** the hashtags. Include the code and the wordcloud itself.

> **Question 1.d**.
> Create key-word-in-context listings for two words from the wordcloud in question **1.c** (without the hashtags). Use the `head` function to only show the top 10 results. Include both listings in your answer, and provide your own interpretation of how the words were used.

Last time we used `dfm_subset` to make a wordcloud just about Obama (`President == Obama`). 
In the current data we also have information about the number of likes for each tweet, which we can use to subset data on only tweets that received at least 100 likes (`likes >= 100`).

> **Question 1.f**.
> Re-create the wordcloud from **1.a** that only uses tweets with at least 100 likes. Include the code and the wordcloud in your answer.


# Assignment 2: Analyzing Reddit

You work for a big company, and are asked to set up a system for monitoring whether and how your company is discussed on Reddit.
As a starting point, you decide to scrape any new threads in the past month. *Note: you don't have to get the comments from the threads, just the titles (otherwise it takes quite long to download)*

> **Question 2.a**.
> Pick a big company or brand (big enough that people would talk about it on Reddit). Collect threads from Reddit in the past month that mention the company, and make a corpus from the titles (`threads$title`).

> **Question 2.b**.
> Create a Document Term Matrix and perform preprocessing. You can yourself decide what preprocessing steps to perform (e.g., removing punctuation, removing stopwords, stemming, subsetting). In addition to providing the code, justify any decision that you made in the preprocessing.

> **Question 2.c**.
> Make a wordcloud from this corpus. Include the code and wordcloud in your answer.

> **Question 1.d**.
> Create key-word-in-context listings for two words from the wordcloud in question **2.c**. Use the `head` function to only show the top 10 results. Include both listings in your answer, and provide your own interpretation of how the words were used.

> **Question 2.d**.
> The threads also mention the subreddit category. Make a wordcloud again, but this time use the subreddit categories instead of the titles


# Writing better code with pipes

When we create a DTM, it's quite inconvenient that we need to first create a corpus, then create the tokens, and then create the DTM.
In particular, it feels unnecessary that we *have* to give names to the corpus and the tokens if we're only interested in the DTM.
And indeed, this actually is unnecessary, and can be avoided by working with `pipes`.

Look again at the following code.

```{r}
corp = corpus(tweets, text_field="text")
tok = tokens(corp)
dtm = dfm(tok)
```

Ideally, we would just provide the `tweets` as input and receive the `dtm` as output, without having to create `corp` and `tok`.
This can be achieved with the `%>%` (pipe) operator, which allows us to create a pipeline of functions.

```{r}
dtm = tweets %>% 
  corpus(text_field="text") %>% 
  tokens() %>% 
  dfm()
```

With this notation, it is immediately clear that these lines of code belong together as a single operation, and no needless names are created.

You can even put parts of the pipe on the same line if you like.

```{r}
dtm = tweets %>% corpus(text_field="text") %>% tokens() %>% dfm()
```

So how does this work? Basically, you should read the pipe operator as `input %>% function`.
The data that comes before the `%>%` operator will be plugged into the function *as the first argument*.
Here are some examples.

```{r}
input = "Example text."

## Pass input to the tokens function
tokens(input)
## Do the exact same thing with a pipe
input %>% tokens()

## Pass input to the tokens function with an additional argument
tokens(input, remove_punct = T)
## Do the exact same thing with a pipe
input %>% tokens(remove_punct = T)
```

You might be wondering why we have two ways of doing the same thing.
The reason is that readability of code is just as important as functionality.
The pipe is not meant to tell R what to do.
The pipe is meant to make it more obvious to a human what the code is doing.
In programming this is also called `syntactic sugar`.

We avoided talking about pipes until now because when you're just getting started 
it's annoying if you have to learn multiple ways to do the same thing. But pipes
can really help you write better code, and if you look up code online you will often
see people using pipes. So it's best to learn it sooner rather than later.

# Assignment 3: Working with pipes

> **Question 3.a**.
> Rewrite the code from question **1.a** using the pipe syntax. 

> **Question 3.b**.
> Rewrite the code from question **1.c** using the pipe syntax.

